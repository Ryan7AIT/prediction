{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/30 12:38:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/30 12:38:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/30 12:38:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/30 12:38:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/30 12:38:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/30 12:38:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/30 12:38:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/04/30 12:38:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+-----+---------+-------------+--------------------+---------+-----------+----------------------+-----------------+-------------------+\n",
      "|thing_id|date_insertion|speed| total_km|engine_status|power_supply_voltage|oil_value|fuel_liters|battery_current_change|daily_avg_voltage|        fuel_change|\n",
      "+--------+--------------+-----+---------+-------------+--------------------+---------+-----------+----------------------+-----------------+-------------------+\n",
      "|     629|    2024-02-28|    0|334365360|            1|               22.25|      3.3|       58.3|                  NULL|          21.3035| 1.7999999999999972|\n",
      "|     629|    2024-02-28|    1|334365360|            1|               22.18|      4.0|       30.0|  -0.07000000000000028|          21.3035|-28.299999999999997|\n",
      "|     629|    2024-02-28|    1|334365364|            1|                22.2|      0.6|       50.7|  0.019999999999999574|          21.3035| 20.700000000000003|\n",
      "|     629|    2024-02-28|    0|334365367|            1|               22.32|      2.8|       38.9|     0.120000000000001|          21.3035|-11.800000000000004|\n",
      "|     629|    2024-02-28|    0|334365368|            1|               22.22|      0.9|        1.4|  -0.10000000000000142|          21.3035|              -37.5|\n",
      "|     629|    2024-02-28|    0|334365368|            0|                22.4|      1.6|       51.3|   0.17999999999999972|          21.3035|               49.9|\n",
      "|     629|    2024-02-28|    0|334365368|            0|                22.3|      4.0|       40.4|  -0.09999999999999787|          21.3035|-10.899999999999999|\n",
      "|     629|    2024-02-28|    0|334365368|            0|                21.3|      2.7|       57.4|                  -1.0|          21.3035|               29.2|\n",
      "|     629|    2024-02-28|    0|334365368|            0|               21.18|      1.0|       11.9|    -0.120000000000001|          21.3035|               -8.9|\n",
      "|     629|    2024-02-28|    0|334365368|            0|               21.11|      1.7|        7.2|  -0.07000000000000028|          21.3035|-35.599999999999994|\n",
      "|     629|    2024-02-28|    0|334365368|            0|               21.04|      2.2|        5.4|  -0.07000000000000028|          21.3035|              -15.4|\n",
      "|     629|    2024-02-28|    0|334365368|            0|               20.97|      2.7|       59.1|  -0.07000000000000028|          21.3035| 2.3000000000000043|\n",
      "|     629|    2024-02-28|    0|334365368|            0|               20.71|      3.0|        9.1|    -0.259999999999998|          21.3035|              -43.6|\n",
      "|     629|    2024-02-28|    0|334365368|            0|               20.67|      1.7|       41.7|  -0.03999999999999915|          21.3035| 31.500000000000004|\n",
      "|     629|    2024-02-28|    0|334365368|            0|               20.63|      0.0|       50.4|   -0.0400000000000027|          21.3035| 20.799999999999997|\n",
      "|     629|    2024-02-28|    0|334365368|            0|               20.59|      0.1|       20.1|  -0.03999999999999915|          21.3035|              -14.0|\n",
      "|     629|    2024-02-28|    0|334365368|            0|               20.55|      3.4|       28.4|  -0.03999999999999915|          21.3035|  2.299999999999997|\n",
      "|     629|    2024-02-28|    0|334365368|            0|               20.52|      3.0|       37.6|  -0.03000000000000...|          21.3035|              -16.1|\n",
      "|     629|    2024-02-28|    0|334365368|            0|               20.48|      0.7|       59.6|  -0.03999999999999915|          21.3035|  2.700000000000003|\n",
      "|     629|    2024-02-28|    0|334365368|            0|               20.45|      3.1|       49.8|  -0.03000000000000...|          21.3035| 3.8999999999999986|\n",
      "+--------+--------------+-----+---------+-------------+--------------------+---------+-----------+----------------------+-----------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, to_date, dayofweek, hour, lag, mean, lit, when,avg\n",
    "from pyspark.sql.window import Window\n",
    "import json\n",
    "from pyspark.sql.functions import rand, isnan, when\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.functions import rand, round\n",
    "from keras.callbacks import TensorBoard\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import datetime\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Initialize a SparkSession\n",
    "spark = SparkSession.builder.appName(\"PredictiveMaintenance\").getOrCreate()\n",
    "\n",
    "# Load your dataset into a PySpark DataFrame\n",
    "df = spark.read.csv('/Users/mac/Downloads/predictive.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Assuming 'date_insertion' is the correct timestamp column based on your dataset schema\n",
    "# Adjust the following transformations accordingly:\n",
    "\n",
    "# Extract JSON fields function\n",
    "def extract_from_json(column, key):\n",
    "    try:\n",
    "        json_data = json.loads(column.replace(\"'\", \"\\\"\"))\n",
    "        return json_data.get(key, None)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Registering the UDF\n",
    "extract_from_json_udf = udf(extract_from_json)\n",
    "\n",
    "# Step 1: Extracting 'oil_value' and 'fuel_liters'\n",
    "df = df.withColumn(\"oil_value\", extract_from_json_udf(col(\"details\"), lit(\"oil_value\")))\n",
    "df = df.withColumn(\"fuel_liters\", extract_from_json_udf(col(\"details\"), lit(\"fuel_liters\")))\n",
    "\n",
    "# Step 2: Creating time-based features\n",
    "df = df.withColumn(\"date_insertion\", to_date(col(\"date_insertion\")))\n",
    "df = df.withColumn(\"day_of_week\", dayofweek(col(\"date_insertion\")))\n",
    "df = df.withColumn(\"hour_of_day\", hour(col(\"date_insertion\")))\n",
    "\n",
    "\n",
    "\n",
    "# Step 4: Aggregate readings on a daily basis\n",
    "daily_avg_df = df.groupBy(\"thing_id\", \"date_insertion\").agg(mean(\"power_supply_voltage\").alias(\"daily_avg_voltage\"))\n",
    "df = df.join(daily_avg_df, [\"thing_id\", \"date_insertion\"], \"left\")\n",
    "\n",
    "# Step 5: Create binary indicator for 'engine_status'\n",
    "df = df.withColumn(\"engine_alert\", when(col(\"engine_status\") == \"Abnormal\", 1).otherwise(0))\n",
    "\n",
    "# Define a UDF to generate random values within a range\n",
    "def random_value(min_value, max_value):\n",
    "    return (rand() * (max_value - min_value) + min_value).cast(FloatType())\n",
    "\n",
    "# random_value_udf = udf(random_value, FloatType())\n",
    "\n",
    "# Set min and max values for 'oil_value' and 'fuel_liters'\n",
    "oil_value_min, oil_value_max = 0, 4\n",
    "fuel_liters_min, fuel_liters_max = 0, 60\n",
    "\n",
    "# Replace null values with random numbers\n",
    "# Replace null values with random numbers and round to 1 decimal place\n",
    "df = df.withColumn(\"oil_value\", when(df['oil_value'].isNull(), round((rand() * (oil_value_max - oil_value_min) + oil_value_min), 1)).otherwise(df['oil_value']))\n",
    "df = df.withColumn(\"fuel_liters\", when(df['fuel_liters'].isNull(), round((rand() * (fuel_liters_max - fuel_liters_min) + fuel_liters_min), 1)).otherwise(df['fuel_liters']))\n",
    "\n",
    "# Step 6: Generate interaction features\n",
    "# df = df.withColumn(\"voltage_current_interaction\", col(\"power_supply_voltage\") * col(\"battery_current\"))\n",
    "\n",
    "# Step 3: Calculating rate of change for 'battery_current'\n",
    "windowSpec = Window.partitionBy(\"thing_id\").orderBy(\"date_insertion\")\n",
    "df = df.withColumn(\"battery_current_change\", col(\"power_supply_voltage\") - lag(\"power_supply_voltage\", 1).over(windowSpec))\n",
    "\n",
    "\n",
    "df = df.select(\"thing_id\", \"date_insertion\", \"speed\", \"total_km\", \"engine_status\", \"power_supply_voltage\" ,\"oil_value\", \"fuel_liters\",  \"battery_current_change\", \"daily_avg_voltage\")\n",
    "\n",
    "\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import lag, avg, stddev\n",
    "\n",
    "# Define a window\n",
    "window = Window.orderBy('date_insertion').rowsBetween(-9, 0)  # assuming 'date_insertion' is your time column\n",
    "\n",
    "# Calculate rolling averages and standard deviations\n",
    "# df = df.withColumn('speed_avg', avg(df['speed']).over(window))\n",
    "# df = df.withColumn('oil_value_std', stddev(df['oil_value']).over(window))\n",
    "\n",
    "# Calculate changes between consecutive readings\n",
    "# df = df.withColumn('speed_change', df['speed'] - lag(df['speed']).over(Window.orderBy('date_insertion')))\n",
    "df = df.withColumn('fuel_change', df['fuel_liters'] - lag(df['fuel_liters']).over(Window.orderBy('date_insertion')))\n",
    "\n",
    "# Step 7: Calculate rolling average\n",
    "\n",
    "\n",
    "# Define a Window specification\n",
    "# windowSpec = Window.orderBy('date_insertion').rowsBetween(-4, 0)  # 5 rows including current row\n",
    "\n",
    "# Calculate rolling average\n",
    "# df = df.withColumn('oil_quality_rolling_avg', avg(df['oil_value']).over(windowSpec))\n",
    "\n",
    "# Show the first 5 rows of the DataFrame\n",
    "df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
